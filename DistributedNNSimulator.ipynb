{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6OL1wLA/VT1tW1zLGqsU8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## A Distributed Neural Network Simulation / How many raspberry pi's would it take to run gpt-4?\n",
        "\n",
        "Distributed Data and Pipeline parallelism for large neural networks are both relatively straightforward concepts. Layer parallelism sounds much more complicated, but is it really so bad? Do you need 256 gpus to understand how this is implemented? Not at all!\n",
        "  \n",
        "Let's see what it looks like to execute a large neural network layer across many machines, and observe the scaling in terms of internal memory usage and bandwidth, and as well as total external network bandwidth. Then let's use these to speculate some interesting scenarios.\n",
        "\n",
        "Todo:\n",
        "- The simulation code is used as a refernce to justify the actual calculations, which are quite simple. These should be extracted because they're useful!\n",
        "- Make a simulation for training, not just inference\n",
        "- Refactor into a virtual machine class to better organize code"
      ],
      "metadata": {
        "id": "Ojsy46cGsHgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qidGhYlQXTf7"
      },
      "outputs": [],
      "source": [
        "from jax import grad, value_and_grad, jit, random, nn\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the parameters and input for our NN"
      ],
      "metadata": {
        "id": "ZeXlFUvnxOTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 30\n",
        "key = random.PRNGKey(seed)\n",
        "embedding_dim = 512\n",
        "nonlinear_dim = 2048\n",
        "batch_size = 6\n",
        "\n",
        "expand_mat = random.normal(key, (embedding_dim, nonlinear_dim)) / nonlinear_dim ** 0.5\n",
        "key, subkey = random.split(key)\n",
        "reduce_mat = random.normal(subkey, (nonlinear_dim, embedding_dim)) / embedding_dim ** 0.5\n",
        "key, subkey = random.split(key)\n",
        "input_vec = random.normal(subkey, (batch_size, embedding_dim,)) / embedding_dim ** 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr09NdjXXdXc",
        "outputId": "45ea89e9-2ce9-48a1-b35c-075895ad4712"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(expand_mat.shape)\n",
        "print(reduce_mat.shape)\n",
        "print(input_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEU8ZEGrYV0l",
        "outputId": "cd47be3a-0f94-494c-85c5-87c58f6c0b06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 2048)\n",
            "(2048, 512)\n",
            "(6, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the layer on a single machine, for reference"
      ],
      "metadata": {
        "id": "riYkV74OxXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_single_machine(x):\n",
        "  return nn.relu(x @ expand_mat) @ reduce_mat"
      ],
      "metadata": {
        "id": "f4yai85GZTvk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for stability by iterating the layer many times (hopefully avoids explosions or vanshing)"
      ],
      "metadata": {
        "id": "GK5cYuy-xt4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_layer(layer_func, x, iterations):\n",
        "  for i in range(iterations):\n",
        "    x = layer_func(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "U6h_XPt3YbG-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_single_machine, input_vec, 1).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M04yxLzGgKFn",
        "outputId": "cad69da7-8506-49d6-a52f-67ac8eff6a29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.12661535, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_single_machine, input_vec, 10).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAW9YLdkY_bN",
        "outputId": "fd1d51a8-1d08-4971-e5fa-7e361851a4db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.09062261, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the layer distribued across N machines. Most of the parameters and computation in a transformer are just these simple MLPs, so they make a fairly good approximation of the whole model."
      ],
      "metadata": {
        "id": "ayn8xIKrx4Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# each machine must use less memory than a single one\n",
        "def layer_n_machines(x, n):\n",
        "  # create virtual machines\n",
        "  reduce_piece_size = embedding_dim // n\n",
        "  expand_piece_size = nonlinear_dim // n\n",
        "  # load/distribute parameters and input across machines\n",
        "  cross_machine_bandwidth = x.size * n\n",
        "  machines = [{\n",
        "      \"input_vec\": x,\n",
        "      \"expand_piece\": expand_mat[:, i*expand_piece_size:(i+1)*expand_piece_size],\n",
        "      \"reduce_piece\": reduce_mat[:, i*reduce_piece_size:(i+1)*reduce_piece_size]\n",
        "      } for i in range(n)]\n",
        "\n",
        "  data_type_bytes = 2 # lets assume fp16\n",
        "  total_params = expand_mat.size + reduce_mat.size\n",
        "  machine_params = machines[0]['expand_piece'].size + machines[0]['expand_piece'].size\n",
        "\n",
        "  # this loop would run in paralell but we'll simulate them serially\n",
        "  for m in machines:\n",
        "    m[\"activation_piece\"] = nn.relu(m[\"input_vec\"] @ m[\"expand_piece\"])\n",
        "  internal_machine_bandwidth = (machines[0][\"expand_piece\"].size + machines[0][\"input_vec\"].size) * n\n",
        "\n",
        "  # collect first matmul results (this requires a small 'all to all' communication)\n",
        "  machines[0][\"full_activation\"] = jnp.hstack([m[\"activation_piece\"] for m in machines])\n",
        "  # no data is actually moved here but pretend that it is\n",
        "  for m in machines:\n",
        "    m[\"full_activation\"] = machines[0][\"full_activation\"]\n",
        "  cross_machine_bandwidth += machines[0][\"full_activation\"].size * n\n",
        "\n",
        "  # project back to embedding for next layer\n",
        "  for m in machines:\n",
        "    m[\"output_piece\"] = m[\"full_activation\"] @ m[\"reduce_piece\"]\n",
        "  internal_machine_bandwidth += (machines[0][\"reduce_piece\"].size + machines[0][\"full_activation\"].size) * n\n",
        "\n",
        "  machines[0][\"full_output\"] = jnp.hstack([m[\"output_piece\"] for m in machines])\n",
        "  cross_machine_bandwidth += machines[0][\"full_output\"].size\n",
        "\n",
        "  assumed_context_size = 1024\n",
        "\n",
        "  info = {\n",
        "      \"batch_size\": x.shape[0],\n",
        "      \"embed_dim\": x.shape[1],\n",
        "      \"activation_dim\": reduce_mat.shape[0],\n",
        "      \"virtual_machines\": n,\n",
        "      \"total_params\": total_params,\n",
        "      \"machine_params\": machine_params,\n",
        "      \"fraction\": machine_params / total_params,\n",
        "      \"internal_bandwidth_params\": internal_machine_bandwidth,\n",
        "      \"cross_machine_bandwidth_params\": cross_machine_bandwidth,\n",
        "      \"internal_bandwidth\": f\"{internal_machine_bandwidth * data_type_bytes / (1000*1000)} mb\",\n",
        "      \"cross_machine_bandwidth\": f\"{cross_machine_bandwidth * data_type_bytes / (1000*1000)} mb\",\n",
        "      # WIP\n",
        "      \"kv_cache_memory\": x.shape[1] * x.shape[0] * 2 * assumed_context_size,\n",
        "      \"attention_cross_machine_bandwidth\": x.shape[1] * x.shape[0] * 2 * assumed_context_size\n",
        "      #\"machine_memory\": machines\n",
        "  }\n",
        "\n",
        "  return machines[0][\"full_output\"], info\n",
        "  # return machines[0][\"full_output\"], machines"
      ],
      "metadata": {
        "id": "YqGqmHCGbWaI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, info = layer_n_machines(input_vec, 4)\n",
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZyCqM5YmhM5",
        "outputId": "019136df-21b8-41fa-d33e-2eeda38297ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQHSEDM0nspf",
        "outputId": "4c661342-ea03-4963-a3f3-e6f225a73054"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 6,\n",
              " 'embed_dim': 512,\n",
              " 'activation_dim': 2048,\n",
              " 'virtual_machines': 4,\n",
              " 'total_params': 2097152,\n",
              " 'machine_params': 524288,\n",
              " 'fraction': 0.25,\n",
              " 'internal_bandwidth_params': 2158592,\n",
              " 'cross_machine_bandwidth_params': 64512,\n",
              " 'internal_bandwidth': '4.317184 mb',\n",
              " 'cross_machine_bandwidth': '0.129024 mb',\n",
              " 'kv_cache_memory': 6291456,\n",
              " 'attention_cross_machine_bandwidth': 6291456}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, info = layer_n_machines(random.normal(key, (64, embedding_dim,)) / embedding_dim ** 0.5, 4)\n",
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2yHCyz4v0AE",
        "outputId": "f6dd3d51-7d9e-4a81-c324-0c06ab678d89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'embed_dim': 512,\n",
              " 'activation_dim': 2048,\n",
              " 'virtual_machines': 4,\n",
              " 'total_params': 2097152,\n",
              " 'machine_params': 524288,\n",
              " 'fraction': 0.25,\n",
              " 'internal_bandwidth_params': 2752512,\n",
              " 'cross_machine_bandwidth_params': 688128,\n",
              " 'internal_bandwidth': '5.505024 mb',\n",
              " 'cross_machine_bandwidth': '1.376256 mb',\n",
              " 'kv_cache_memory': 67108864,\n",
              " 'attention_cross_machine_bandwidth': 67108864}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing batch size comes almost for free!"
      ],
      "metadata": {
        "id": "91cUtI_YuGNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result, info = layer_n_machines(input_vec, 128)\n",
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBZDFSV9s-3P",
        "outputId": "72875d84-7b64-44b8-b7f9-1b31f8f0c2b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 6,\n",
              " 'embed_dim': 512,\n",
              " 'activation_dim': 2048,\n",
              " 'virtual_machines': 128,\n",
              " 'total_params': 2097152,\n",
              " 'machine_params': 16384,\n",
              " 'fraction': 0.0078125,\n",
              " 'internal_bandwidth_params': 4063232,\n",
              " 'cross_machine_bandwidth_params': 1969152,\n",
              " 'internal_bandwidth': '8.126464 mb',\n",
              " 'cross_machine_bandwidth': '3.938304 mb',\n",
              " 'kv_cache_memory': 6291456,\n",
              " 'attention_cross_machine_bandwidth': 6291456}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Internal memory usage and bandwidth scale beautifully!\n",
        "In this regime cross machine networking becomes critical!\n",
        "  \n"
      ],
      "metadata": {
        "id": "HQGTHj1BwFJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_four_machines(x):\n",
        "  return layer_n_machines(x, 4)[0]"
      ],
      "metadata": {
        "id": "O8n4ptfTqSb0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_eight_machines(x):\n",
        "  return layer_n_machines(x, 8)[0]"
      ],
      "metadata": {
        "id": "Hc3VJBWqsglB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_lots_o_machines(x):\n",
        "  return layer_n_machines(x, 256)[0]"
      ],
      "metadata": {
        "id": "ebMYRYNqstXZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_single_machine, input_vec, 10).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAO0V24HrwXr",
        "outputId": "5825381e-db04-461e-cbcb-24310847c233"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.09062261, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_four_machines, input_vec, 10).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fapcSxlsryn6",
        "outputId": "bddebe16-2029-4fdc-ba9a-8b87571a6068"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.09062257, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_eight_machines, input_vec, 10).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qI7-aBpspMg",
        "outputId": "b8734283-2b87-4b2a-f02d-0eeb5d47972f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.09062257, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_layer(layer_lots_o_machines, input_vec, 10).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPT7Y_Csx4Z",
        "outputId": "dc1e761f-1417-48d8-827f-3976397250d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(-0.0906228, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of our simulations match the single machine version, yay!"
      ],
      "metadata": {
        "id": "4U98WdoswkmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlPwI5_4sFqr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make some more assumptions to roughly approximate gpt-4. Let's assume 1700 billion fp16 parameters, split across 8 gpt-3ish experts, each with 192 layers with embedding dim 12288 and an activation dim 4x that. We're just going for a very rough ballpark."
      ],
      "metadata": {
        "id": "umKnWb2t0TUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4_params = 12288 * (12288 * 4) * 2 * 192 * 8\n",
        "f\"{gpt4_params / 1000000000000} trillion params\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iz3_wXwVzBjU",
        "outputId": "a3f8bb85-7ed5-4146-a35f-0a7f7a93116d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.855425871872 trillion params'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Close enough :)"
      ],
      "metadata": {
        "id": "sUhzAD0g2CvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically if you attached a large storage device, a single raspberry pi could run gpt-4 but it would be quite slow (though this is another separate interesting question). Context size and kv cache would cause attention to be a significant factor for memory and bandwidth when using large batch sizes, but for batch size of 1 they are negligable.  Raspberry pi can have 4gb of ram. So we'll need:"
      ],
      "metadata": {
        "id": "0QKPPr-m3R2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_pis = (gpt4_params / 1000000000) * 2 // 4\n",
        "n_pis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Ht1RVj3Y-J",
        "outputId": "e1ec2313-73f0-4ac1-ccf4-dbb05a310b2f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "927.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's round up to 1024"
      ],
      "metadata": {
        "id": "v3c06s8r4Jp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_pies = 1024"
      ],
      "metadata": {
        "id": "bJAgVCmq4NYd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 12288\n",
        "nonlinear_dim = 12288 * 4\n",
        "batch_size = 1\n",
        "\n",
        "expand_mat = random.normal(key, (embedding_dim, nonlinear_dim)) / nonlinear_dim ** 0.5\n",
        "key, subkey = random.split(key)\n",
        "reduce_mat = random.normal(subkey, (nonlinear_dim, embedding_dim)) / embedding_dim ** 0.5\n",
        "key, subkey = random.split(key)\n",
        "input_vec = random.normal(subkey, (batch_size, embedding_dim,)) / embedding_dim ** 0.5"
      ],
      "metadata": {
        "id": "G8jtB2xb2eZ-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, info = layer_n_machines(input_vec, n_pies)"
      ],
      "metadata": {
        "id": "TlQzM4Eb4b-U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6FCN46H4P9E",
        "outputId": "a156db81-b437-4fa4-915f-d05138574ec6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1,\n",
              " 'embed_dim': 12288,\n",
              " 'activation_dim': 49152,\n",
              " 'virtual_machines': 1024,\n",
              " 'total_params': 1207959552,\n",
              " 'machine_params': 1179648,\n",
              " 'fraction': 0.0009765625,\n",
              " 'internal_bandwidth_params': 1270874112,\n",
              " 'cross_machine_bandwidth_params': 62926848,\n",
              " 'internal_bandwidth': '2541.748224 mb',\n",
              " 'cross_machine_bandwidth': '125.853696 mb',\n",
              " 'kv_cache_memory': 25165824,\n",
              " 'attention_cross_machine_bandwidth': 25165824}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can multiply these figures by 192 to account for all layers in a forward pass, and again by 8 for each of the eight experts"
      ],
      "metadata": {
        "id": "RF3LJbDH6Fgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{info['machine_params'] * 192 * 8 / 1_000_000_000:.3f}B params on each pi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KUpQro8J56Nb",
        "outputId": "133dcd2d-c5c0-420a-8747-b2077fd0024b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.812B params on each pi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For bandwidth calculations, we'll assume only 2 experts are activated at a time. But we can't know these in advance so the experts that are acitvate will be using their underlying resources at full capacity. This is WIP"
      ],
      "metadata": {
        "id": "p5p1qoyo8Lyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{info['internal_bandwidth_params'] * 192 * 8 * 2 / 1000000000:.3f}B internal memory use on each pi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RyQYKL8k6qik",
        "outputId": "5f371991-6cb4-430d-8182-f33e215767c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3904.125B internal memory use on each pi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv8fes1_rAiU"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}